{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(\n",
        "    temperature = 0,\n",
        "    groq_api_key = \"\",\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")\n",
        "result = llm.invoke(\"Who is lord Ram?\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qqqZDKUn_MH",
        "outputId": "e494366a-0513-4b38-9f52-7cbb98da26ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lord Rama is a major deity in Hinduism and is considered one of the most revered and beloved gods in the Hindu pantheon. He is the seventh avatar (incarnation) of Lord Vishnu, the preserver of the universe, and is often referred to as Maryada Purushottam, which means \"the perfect man\" or \"the embodiment of righteousness.\"\n",
            "\n",
            "According to Hindu mythology, Lord Rama was born in the city of Ayodhya, in the kingdom of Kosala, to King Dasharatha and Queen Kausalya. He was the eldest of four brothers, including Lakshmana, Bharata, and Shatrughna.\n",
            "\n",
            "The story of Lord Rama is told in the epic poem, the Ramayana, which is one of the most sacred texts in Hinduism. The Ramayana describes Rama's life, including his exile to the forest, his battle with the demon king Ravana, and his eventual return to Ayodhya to reclaim his throne.\n",
            "\n",
            "Lord Rama is revered for his many virtues, including:\n",
            "\n",
            "1. **Dharma**: Rama is considered the embodiment of dharma, or righteousness. He is known for his unwavering commitment to truth, justice, and morality.\n",
            "2. **Bravery**: Rama is a skilled warrior and is known for his bravery in battle. He fought against many powerful demons and ultimately defeated Ravana, the king of Lanka.\n",
            "3. **Compassion**: Rama is known for his compassion and empathy towards all living beings. He is often depicted as a just and fair ruler, who cared for the welfare of his people.\n",
            "4. **Devotion**: Rama is deeply devoted to his family, particularly his wife Sita and his brother Lakshmana. He is also devoted to his duty as a king and a warrior.\n",
            "\n",
            "The worship of Lord Rama is widespread in India and other parts of the world, and his story has been retold and reinterpreted in many different ways. He is often depicted in art and literature as a handsome, strong, and wise king, with a blue complexion and a bow and arrow.\n",
            "\n",
            "Some of the key events in Lord Rama's life include:\n",
            "\n",
            "* His birth in Ayodhya and his coronation as the crown prince\n",
            "* His exile to the forest, where he lived with his wife Sita and his brother Lakshmana\n",
            "* The abduction of Sita by Ravana, which led to the Battle of Lanka\n",
            "* The defeat of Ravana and the recovery of Sita\n",
            "* Rama's return to Ayodhya and his coronation as the king\n",
            "\n",
            "Overall, Lord Rama is a complex and multifaceted figure, who embodies many of the values and ideals that are central to Hinduism. His story has been a source of inspiration and guidance for millions of people around the world, and continues to be celebrated and revered to this day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "def initialize_llm():\n",
        "  llm = ChatGroq(\n",
        "    temperature = 0,\n",
        "    groq_api_key = \"\",\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")\n",
        "  return llm\n",
        "\n",
        "def create_vector_db():\n",
        "  loader = DirectoryLoader(\"/content/data/\", glob = '*.pdf', loader_cls = PyPDFLoader)\n",
        "  documents = loader.load()\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')\n",
        "  vector_db.persist()\n",
        "\n",
        "  print(\"ChromaDB created and data saved\")\n",
        "\n",
        "  return vector_db\n",
        "\n",
        "def setup_qa_chain(vector_db, llm):\n",
        "  retriever = vector_db.as_retriever()\n",
        "  prompt_templates = \"\"\" You are a compassionate mental health chatbot. Respond thoughtfully to the following question:\n",
        "    {context}\n",
        "    User: {question}\n",
        "    Chatbot: \"\"\"\n",
        "  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "      llm = llm,\n",
        "      chain_type = \"stuff\",\n",
        "      retriever = retriever,\n",
        "      chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "  )\n",
        "  return qa_chain\n",
        "\n",
        "\n",
        "def main():\n",
        "  print(\"Intializing Chatbot.........\")\n",
        "  llm = initialize_llm()\n",
        "\n",
        "  db_path = \"/content/chroma_db\"\n",
        "\n",
        "  if not os.path.exists(db_path):\n",
        "    vector_db  = create_vector_db()\n",
        "  else:\n",
        "    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "  qa_chain = setup_qa_chain(vector_db, llm)\n",
        "\n",
        "  while True:\n",
        "    query = input(\"\\nHuman: \")\n",
        "    if query.lower()  == \"exit\":\n",
        "      print(\"Chatbot: Take Care of yourself, Goodbye!\")\n",
        "      break\n",
        "    response = qa_chain.run(query)\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI6N0jVCoFP1",
        "outputId": "da5695cf-9c18-466e-bbf6-18f587d13da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intializing Chatbot.........\n",
            "\n",
            "Human: exit\n",
            "Chatbot: Take Care of yourself, Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import gradio as gr\n",
        "def initialize_llm():\n",
        "  llm = ChatGroq(\n",
        "    temperature = 0,\n",
        "    groq_api_key = \"\",\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")\n",
        "  return llm\n",
        "\n",
        "def create_vector_db():\n",
        "  loader = DirectoryLoader(\"/content/data/\", glob = '*.pdf', loader_cls = PyPDFLoader)\n",
        "  documents = loader.load()\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')\n",
        "  vector_db.persist()\n",
        "\n",
        "  print(\"ChromaDB created and data saved\")\n",
        "\n",
        "  return vector_db\n",
        "\n",
        "def setup_qa_chain(vector_db, llm):\n",
        "  retriever = vector_db.as_retriever()\n",
        "  prompt_templates = \"\"\" You are a compassionate mental health chatbot. Respond thoughtfully to the following question:\n",
        "    {context}\n",
        "    User: {question}\n",
        "    Chatbot: \"\"\"\n",
        "  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "      llm = llm,\n",
        "      chain_type = \"stuff\",\n",
        "      retriever = retriever,\n",
        "      chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "  )\n",
        "  return qa_chain\n",
        "\n",
        "\n",
        "print(\"Intializing Chatbot.........\")\n",
        "llm = initialize_llm()\n",
        "\n",
        "db_path = \"/content/chroma_db\"\n",
        "\n",
        "if not os.path.exists(db_path):\n",
        "  vector_db  = create_vector_db()\n",
        "else:\n",
        "  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "qa_chain = setup_qa_chain(vector_db, llm)\n",
        "\n",
        "import traceback\n",
        "\n",
        "def chatbot_response(user_input, history):\n",
        "    if not user_input.strip():\n",
        "        return \"Please provide a valid input\"\n",
        "    response = qa_chain.run(user_input)\n",
        "    return response\n",
        "\n",
        "with gr.Blocks(theme = 'Respair/Shiki@1.2.1') as app:\n",
        "    gr.Markdown(\"# ðŸ§  Mental Health Chatbot ðŸ¤–\")\n",
        "    gr.Markdown(\"A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.\")\n",
        "\n",
        "    chatbot = gr.ChatInterface(fn=chatbot_response, title=\"Mental Health Chatbot\")\n",
        "\n",
        "    gr.Markdown(\"This chatbot provides general support. For urgent issues, seek help from licensed professionals.\")\n",
        "\n",
        "app.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "Ixmn8JBuoK_f",
        "outputId": "74b45147-dbae-449b-cb3c-6438c0ee269c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intializing Chatbot.........\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3c79839a82316ed9d2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3c79839a82316ed9d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7866 <> https://3c79839a82316ed9d2.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXFY71kMoR4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}